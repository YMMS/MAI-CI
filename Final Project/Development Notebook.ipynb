{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "import os\n",
    "import configparser\n",
    "import webbrowser\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "os.chdir(\"C:/Users/macle/Desktop/UPC Masters/Semester 2/CI/Final Project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('secrets.ini')\n",
    "\n",
    "reddit_client_id = config.get('reddit', 'client_id')\n",
    "reddit_api_key = config.get('reddit', 'api_key')\n",
    "r.set_oauth_app_info(client_id=reddit_client_id,\n",
    "                      client_secret=reddit_api_key,\n",
    "                      redirect_uri='http://cole-maclean.github.io/MAI-CI/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = r.get_authorize_url('uniqueKey', 'identity', True)\n",
    "webbrowser.open(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "access_information = r.get_access_information('B3w4W9k5xFbJpaLv8Xpuf-iQj34')\n",
    "r.set_access_credentials(**access_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upcmaici 1\n"
     ]
    }
   ],
   "source": [
    "authenticated_user = r.get_me()\n",
    "print(authenticated_user.name, authenticated_user.link_karma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrapping 0th subreddit\n",
      "scrapping users from subreddit r/news\n",
      "scrapping 1th subreddit\n",
      "scrapping users from subreddit r/videos\n",
      "scrapping 2th subreddit\n",
      "scrapping users from subreddit r/grimfandango\n",
      "scrapping 3th subreddit\n",
      "scrapping users from subreddit r/streetwear\n",
      "scrapping 4th subreddit\n",
      "scrapping users from subreddit r/Foodforthought\n",
      "scrapping 5th subreddit\n",
      "scrapping users from subreddit r/hearthstone\n",
      "scrapping 6th subreddit\n",
      "scrapping users from subreddit r/aussievapers\n",
      "scrapping 7th subreddit\n",
      "scrapping users from subreddit r/technology\n",
      "scrapping 8th subreddit\n",
      "scrapping users from subreddit r/dirtykikpals\n",
      "scrapping 9th subreddit\n",
      "scrapping users from subreddit r/futurama\n"
     ]
    }
   ],
   "source": [
    "with open('reddit_data.json','r') as data_file:    \n",
    "    reddit_data = json.load(data_file)\n",
    "with open('scrapped_users.json','r') as data_file:    \n",
    "    scrapped_users = json.load(data_file)\n",
    "for i in range(10):\n",
    "    print (\"scrapping \" + str(i) + \"th subreddit\")\n",
    "    rand_submission = r.get_random_submission()\n",
    "    print (\"scrapping users from subreddit r/\" + rand_submission.subreddit.display_name)\n",
    "    if len(rand_submission.comments) >=3:\n",
    "        rnd_comments = random.sample(rand_submission.comments,min(10,len(rand_submission.comments)))\n",
    "        for comment in rnd_comments:\n",
    "            if isinstance(comment, praw.objects.Comment):\n",
    "                user = comment.author\n",
    "                if user:\n",
    "                    if user.name in scrapped_users:\n",
    "                        print ('user ' + user.name + 'already scraped')\n",
    "                    else:\n",
    "                        scrapped_users.append(user.name)\n",
    "                        for user_comment in user.get_comments(limit=None,_use_oauth=False):\n",
    "                            body = user_comment.body.split()\n",
    "                            if len(body) >= 10:\n",
    "                                rand_words = random.sample(body,10)\n",
    "                            else:\n",
    "                                rand_words = body\n",
    "                            reddit_data.append([user_comment.subreddit.display_name,\n",
    "                                                                              user_comment.created_utc,rand_words])\n",
    "                    \n",
    "with open('reddit_data.json','w') as data_file:\n",
    "    json.dump(reddit_data, data_file)\n",
    "with open('scrapped_users.json','w') as data_file:\n",
    "    json.dump(scrapped_users, data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>utc_stamp</th>\n",
       "      <th>rnd_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>breathinginformation</td>\n",
       "      <td>1479531643</td>\n",
       "      <td>[beat, Griffon, the, ball, when, in, Peggle, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baseball</td>\n",
       "      <td>1479531569</td>\n",
       "      <td>[Hopefully, both, deals.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>baseball</td>\n",
       "      <td>1479530961</td>\n",
       "      <td>[Rodon, and, Gonzalez, 1-2., Shields, 3.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>baseball</td>\n",
       "      <td>1479530947</td>\n",
       "      <td>[I, count, Rodon, and, Miguel, Gonzalez, befor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>breathinginformation</td>\n",
       "      <td>1479527248</td>\n",
       "      <td>[Ryan, dropped, old, in, school., knife, Wow, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              subreddit   utc_stamp  \\\n",
       "0  breathinginformation  1479531643   \n",
       "1              baseball  1479531569   \n",
       "2              baseball  1479530961   \n",
       "3              baseball  1479530947   \n",
       "4  breathinginformation  1479527248   \n",
       "\n",
       "                                           rnd_words  \n",
       "0  [beat, Griffon, the, ball, when, in, Peggle, d...  \n",
       "1                          [Hopefully, both, deals.]  \n",
       "2          [Rodon, and, Gonzalez, 1-2., Shields, 3.]  \n",
       "3  [I, count, Rodon, and, Miguel, Gonzalez, befor...  \n",
       "4  [Ryan, dropped, old, in, school., knife, Wow, ...  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(reddit_data,columns=['subreddit','utc_stamp','rnd_words'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = {'subreddit':['count'], 'utc_stamp':[np.min,np.max],'rnd_words':['count']}\n",
    "grouped = df.groupby('subreddit').agg(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_of_words = []\n",
    "translate_table = dict((ord(char), None) for char in string.punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "for user_comment in reddit_data:\n",
    "    for word in user_comment[2]:\n",
    "        if len(word) < 45:\n",
    "            clean_word = word.translate(translate_table)\n",
    "            wrd_stm = stemmer.stem(clean_word)\n",
    "            list_of_words.append(wrd_stm)\n",
    "word_counts = collections.Counter(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "741"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reddit_data.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
