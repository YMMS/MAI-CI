{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "import SubRecommender as sr\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'SubRecommender' from 'C:\\\\Users\\\\macle\\\\Desktop\\\\UPC Masters\\\\Semester 2\\\\CI\\\\SubRecommender\\\\SubRecommender.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tst = sr.SubRecommender(train_data_file=\"data/train_reddit_data.json\",save_model_file=\"models/tst\",sequence_chunk_size=51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Training Data\n",
      "Building Vocab\n",
      "Building Training Sequences\n",
      "Sequence Builder 0.0 % Complete\n",
      "Sequence Builder 10.0 % Complete\n",
      "Sequence Builder 20.0 % Complete\n",
      "Sequence Builder 30.0 % Complete\n",
      "Sequence Builder 40.0 % Complete\n",
      "Sequence Builder 50.0 % Complete\n",
      "Sequence Builder 60.0 % Complete\n",
      "Sequence Builder 70.0 % Complete\n",
      "Sequence Builder 80.0 % Complete\n",
      "Sequence Builder 90.0 % Complete\n",
      "Sequence Builder 100.0 % Complete\n",
      "Building Graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Network\n",
      "Accuracy after epoch 1  - tr: 0.154789719626 - te: 0.229600694444\n",
      "Accuracy after epoch 2  - tr: 0.0445533608491 - te: 0.115985576923\n",
      "Accuracy after epoch 3  - tr: 0.0675486438679 - te: 0.101262019231\n",
      "Accuracy after epoch 4  - tr: 0.0942659198113 - te: 0.110727163462\n",
      "Accuracy after epoch 5  - tr: 0.112065153302 - te: 0.152644230769\n",
      "Accuracy after epoch 6  - tr: 0.128795695755 - te: 0.169771634615\n",
      "Accuracy after epoch 7  - tr: 0.147626768868 - te: 0.192457932692\n",
      "Accuracy after epoch 8  - tr: 0.168890035377 - te: 0.225961538462\n",
      "Accuracy after epoch 9  - tr: 0.187757959906 - te: 0.254507211538\n",
      "Accuracy after epoch 10  - tr: 0.210126768868 - te: 0.267878605769\n",
      "Accuracy after epoch 11  - tr: 0.227225825472 - te: 0.276893028846\n",
      "Accuracy after epoch 12  - tr: 0.249078714623 - te: 0.284555288462\n",
      "Accuracy after epoch 13  - tr: 0.263929834906 - te: 0.304086538462\n",
      "Accuracy after epoch 14  - tr: 0.279112617925 - te: 0.326322115385\n",
      "Accuracy after epoch 15  - tr: 0.290905070755 - te: 0.337139423077\n",
      "Accuracy after epoch 16  - tr: 0.305056014151 - te: 0.351862980769\n",
      "Accuracy after epoch 17  - tr: 0.318691037736 - te: 0.373197115385\n",
      "Accuracy after epoch 18  - tr: 0.334573997642 - te: 0.386868990385\n",
      "Accuracy after epoch 19  - tr: 0.346440153302 - te: 0.414513221154\n",
      "Accuracy after epoch 20  - tr: 0.362949587264 - te: 0.422325721154\n",
      "Accuracy after epoch 21  - tr: 0.374705188679 - te: 0.442307692308\n",
      "Accuracy after epoch 22  - tr: 0.385686910377 - te: 0.451772836538\n",
      "Accuracy after epoch 23  - tr: 0.396263266509 - te: 0.466796875\n",
      "Accuracy after epoch 24  - tr: 0.410414209906 - te: 0.472806490385\n",
      "Accuracy after epoch 25  - tr: 0.41999557783 - te: 0.48046875\n",
      "Accuracy after epoch 26  - tr: 0.429576945755 - te: 0.489182692308\n",
      "Accuracy after epoch 27  - tr: 0.438384433962 - te: 0.493990384615\n",
      "Accuracy after epoch 28  - tr: 0.447449882075 - te: 0.501201923077\n",
      "Accuracy after epoch 29  - tr: 0.458800117925 - te: 0.512469951923\n",
      "Accuracy after epoch 30  - tr: 0.468639445755 - te: 0.516376201923\n",
      "Accuracy after epoch 31  - tr: 0.475899174528 - te: 0.523888221154\n",
      "Accuracy after epoch 32  - tr: 0.483122051887 - te: 0.532151442308\n",
      "Accuracy after epoch 33  - tr: 0.493661556604 - te: 0.543419471154\n",
      "Accuracy after epoch 34  - tr: 0.499520931604 - te: 0.549879807692\n",
      "Accuracy after epoch 35  - tr: 0.50961821934 - te: 0.559044471154\n",
      "Accuracy after epoch 36  - tr: 0.517062205189 - te: 0.564603365385\n",
      "Accuracy after epoch 37  - tr: 0.52329009434 - te: 0.567157451923\n",
      "Accuracy after epoch 38  - tr: 0.529112617925 - te: 0.576622596154\n",
      "Accuracy after epoch 39  - tr: 0.539357311321 - te: 0.582481971154\n",
      "Accuracy after epoch 40  - tr: 0.543816332547 - te: 0.593599759615\n",
      "Accuracy after epoch 41  - tr: 0.553692511792 - te: 0.603966346154\n",
      "Accuracy after epoch 42  - tr: 0.558925412736 - te: 0.607421875\n",
      "Accuracy after epoch 43  - tr: 0.56625884434 - te: 0.616586538462\n",
      "Accuracy after epoch 44  - tr: 0.573223761792 - te: 0.618539663462\n",
      "Accuracy after epoch 45  - tr: 0.578788325472 - te: 0.618539663462\n",
      "Accuracy after epoch 46  - tr: 0.584242334906 - te: 0.635066105769\n",
      "Accuracy after epoch 47  - tr: 0.590322818396 - te: 0.629807692308\n",
      "Accuracy after epoch 48  - tr: 0.597435141509 - te: 0.635216346154\n",
      "Accuracy after epoch 49  - tr: 0.601599351415 - te: 0.642277644231\n",
      "Accuracy after epoch 50  - tr: 0.60922759434 - te: 0.650540865385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.15478971962616822,\n",
       "  0.044553360849056603,\n",
       "  0.067548643867924529,\n",
       "  0.09426591981132075,\n",
       "  0.11206515330188679,\n",
       "  0.12879569575471697,\n",
       "  0.14762676886792453,\n",
       "  0.1688900353773585,\n",
       "  0.18775795990566038,\n",
       "  0.21012676886792453,\n",
       "  0.22722582547169812,\n",
       "  0.2490787146226415,\n",
       "  0.26392983490566035,\n",
       "  0.27911261792452829,\n",
       "  0.290905070754717,\n",
       "  0.30505601415094341,\n",
       "  0.31869103773584906,\n",
       "  0.33457399764150941,\n",
       "  0.34644015330188677,\n",
       "  0.36294958726415094,\n",
       "  0.37470518867924529,\n",
       "  0.38568691037735847,\n",
       "  0.39626326650943394,\n",
       "  0.41041420990566035,\n",
       "  0.4199955778301887,\n",
       "  0.429576945754717,\n",
       "  0.43838443396226418,\n",
       "  0.44744988207547171,\n",
       "  0.45880011792452829,\n",
       "  0.468639445754717,\n",
       "  0.47589917452830188,\n",
       "  0.48312205188679247,\n",
       "  0.49366155660377359,\n",
       "  0.49952093160377359,\n",
       "  0.50961821933962259,\n",
       "  0.51706220518867929,\n",
       "  0.52329009433962259,\n",
       "  0.52911261792452835,\n",
       "  0.53935731132075471,\n",
       "  0.54381633254716977,\n",
       "  0.55369251179245282,\n",
       "  0.55892541273584906,\n",
       "  0.56625884433962259,\n",
       "  0.57322376179245282,\n",
       "  0.57878832547169812,\n",
       "  0.58424233490566035,\n",
       "  0.59032281839622647,\n",
       "  0.597435141509434,\n",
       "  0.60159935141509435,\n",
       "  0.60922759433962259],\n",
       " [0.22960069444444445,\n",
       "  0.11598557692307693,\n",
       "  0.10126201923076923,\n",
       "  0.11072716346153846,\n",
       "  0.15264423076923078,\n",
       "  0.16977163461538461,\n",
       "  0.19245793269230768,\n",
       "  0.22596153846153846,\n",
       "  0.25450721153846156,\n",
       "  0.26787860576923078,\n",
       "  0.27689302884615385,\n",
       "  0.28455528846153844,\n",
       "  0.30408653846153844,\n",
       "  0.32632211538461536,\n",
       "  0.33713942307692307,\n",
       "  0.35186298076923078,\n",
       "  0.37319711538461536,\n",
       "  0.38686899038461536,\n",
       "  0.41451322115384615,\n",
       "  0.42232572115384615,\n",
       "  0.44230769230769229,\n",
       "  0.45177283653846156,\n",
       "  0.466796875,\n",
       "  0.47280649038461536,\n",
       "  0.48046875,\n",
       "  0.48918269230769229,\n",
       "  0.49399038461538464,\n",
       "  0.50120192307692313,\n",
       "  0.51246995192307687,\n",
       "  0.51637620192307687,\n",
       "  0.52388822115384615,\n",
       "  0.53215144230769229,\n",
       "  0.54341947115384615,\n",
       "  0.54987980769230771,\n",
       "  0.55904447115384615,\n",
       "  0.56460336538461542,\n",
       "  0.56715745192307687,\n",
       "  0.57662259615384615,\n",
       "  0.58248197115384615,\n",
       "  0.59359975961538458,\n",
       "  0.60396634615384615,\n",
       "  0.607421875,\n",
       "  0.61658653846153844,\n",
       "  0.61853966346153844,\n",
       "  0.61853966346153844,\n",
       "  0.63506610576923073,\n",
       "  0.62980769230769229,\n",
       "  0.63521634615384615,\n",
       "  0.64227764423076927,\n",
       "  0.65054086538461542])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst.train_network(num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10701, 14952, 17799, 14442, 5835, 7775, 14952, 15704, 14952, 8868, 754, 17756, 13093, 12982, 14952, 2557, 754, 14952, 754, 14952, 754, 14952, 17045, 13099, 14952, 12982, 12894, 14952, 17756, 14952, 2557, 14952, 754, 2557, 14952, 754, 14952, 754, 14952, 754, 7775, 14952, 754, 15720, 4236, 15720, 8968, 2016, 14952, 754]\n",
      "11224\n"
     ]
    }
   ],
   "source": [
    "print(tst.training_sequences[0])\n",
    "print(tst.training_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34025\n"
     ]
    }
   ],
   "source": [
    "print(len(tst.training_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/test_reddit_data.json','r') as data_file:    \n",
    "    reddit_data = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(reddit_data,columns=['user','subreddit','utc_stamp'])\n",
    "df['utc_stamp'] = pd.to_datetime(df['utc_stamp'],unit='s')\n",
    "df.sort_values(by=['user','utc_stamp'], ascending=True, inplace=True)\n",
    "users = list(df.groupby('user')['user'].nunique().keys())\n",
    "sub_list = list(df.groupby('subreddit')['subreddit'].nunique().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_recs = []\n",
    "for usr in users[0:25]:\n",
    "    user_comment_subs = list(df.loc[df['user'] == usr]['subreddit'].values)\n",
    "    rec_subs = tst.rec_subs(user_comment_subs)\n",
    "    all_recs.append(rec_subs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tattoos', 0.02807638),\n",
       " ('tattoos', 0.029045738),\n",
       " ('tattoos', 0.02807638),\n",
       " ('tattoos', 0.02968988),\n",
       " ('tattoos', 0.02807638),\n",
       " ('tattoos', 0.0292251),\n",
       " ('tattoos', 0.029689876),\n",
       " ('tattoos', 0.029709285),\n",
       " ('tattoos', 0.02807638),\n",
       " ('tattoos', 0.029709285),\n",
       " ('tattoos', 0.029709285),\n",
       " ('tattoos', 0.029709285),\n",
       " ('tattoos', 0.029709281),\n",
       " ('tattoos', 0.02807638),\n",
       " ('tattoos', 0.029709285),\n",
       " ('tattoos', 0.02807638),\n",
       " ('tattoos', 0.028640369),\n",
       " ('tattoos', 0.02807638),\n",
       " ('tattoos', 0.029709281),\n",
       " ('tattoos', 0.029419301),\n",
       " ('tattoos', 0.02807638),\n",
       " ('tattoos', 0.029640719),\n",
       " ('tattoos', 0.029138884),\n",
       " ('tattoos', 0.029709285),\n",
       " ('tattoos', 0.02807638)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tst = sr.SubRecommender(train_data_file=\"data/test_reddit_data.json\",save_model_file=\"models/tst\",sequence_chunk_size=51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Training Data\n",
      "Building Vocab\n",
      "Building Training Sequences\n",
      "Sequence Builder 0.0 % Complete\n",
      "Sequence Builder 10.0 % Complete\n",
      "Sequence Builder 19.0 % Complete\n",
      "Sequence Builder 29.0 % Complete\n",
      "Sequence Builder 38.0 % Complete\n",
      "Sequence Builder 48.0 % Complete\n",
      "Sequence Builder 57.0 % Complete\n",
      "Sequence Builder 67.0 % Complete\n",
      "Sequence Builder 76.0 % Complete\n",
      "Sequence Builder 86.0 % Complete\n",
      "Sequence Builder 95.0 % Complete\n"
     ]
    }
   ],
   "source": [
    "df = tst.load_train_df()\n",
    "train,test = tst.split_train_test(df,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python35\\lib\\site-packages\\tflearn\\summaries.py:46 in get_summary.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From c:\\python35\\lib\\site-packages\\tflearn\\summaries.py:46 in get_summary.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From c:\\python35\\lib\\site-packages\\tflearn\\helpers\\trainer.py:766 in create_summaries.: merge_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:From c:\\python35\\lib\\site-packages\\tflearn\\helpers\\trainer.py:130 in __init__.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "---------------------------------\n",
      "Run id: QM0FAJ\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "WARNING:tensorflow:From c:\\python35\\lib\\site-packages\\tflearn\\helpers\\trainer.py:210 in fit.: SummaryWriter.__init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'NoneType' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing summary_tags.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'NoneType' object has no attribute 'name'\n",
      "WARNING:tensorflow:From c:\\python35\\lib\\site-packages\\tflearn\\summaries.py:46 in get_summary.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From c:\\python35\\lib\\site-packages\\tflearn\\helpers\\summarizer.py:89 in summarize.: merge_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge.\n",
      "WARNING:tensorflow:From c:\\python35\\lib\\site-packages\\tflearn\\summaries.py:46 in get_summary.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From c:\\python35\\lib\\site-packages\\tflearn\\helpers\\summarizer.py:89 in summarize.: merge_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge.\n",
      "WARNING:tensorflow:From c:\\python35\\lib\\site-packages\\tflearn\\summaries.py:46 in get_summary.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From c:\\python35\\lib\\site-packages\\tflearn\\helpers\\summarizer.py:89 in summarize.: merge_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge.\n",
      "WARNING:tensorflow:From c:\\python35\\lib\\site-packages\\tflearn\\summaries.py:46 in get_summary.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From c:\\python35\\lib\\site-packages\\tflearn\\helpers\\summarizer.py:89 in summarize.: merge_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge.\n",
      "---------------------------------\n",
      "Training samples: 470\n",
      "Validation samples: 118\n",
      "--\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (32, 50) for Tensor 'InputData/X:0', which has shape '(?, 51)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-efde90184a37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensorboard_verbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m model.fit(trainX, trainY, validation_set=(testX, testY), show_metric=True,\n\u001b[0;32m---> 44\u001b[0;31m           batch_size=32)\n\u001b[0m",
      "\u001b[0;32mc:\\python35\\lib\\site-packages\\tflearn\\models\\dnn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, run_id)\u001b[0m\n\u001b[1;32m    186\u001b[0m                          \u001b[0mdaug_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdaug_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                          \u001b[0mexcl_trainops\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexcl_trainops\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                          run_id=run_id)\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\python35\\lib\\site-packages\\tflearn\\helpers\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, feed_dicts, n_epoch, val_feed_dicts, show_metric, snapshot_step, snapshot_epoch, shuffle_all, dprep_dict, daug_dict, excl_trainops, run_id)\u001b[0m\n\u001b[1;32m    275\u001b[0m                                                        \u001b[0msnapshot_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                                                        \u001b[0msnapshot_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                                                        show_metric)\n\u001b[0m\u001b[1;32m    278\u001b[0m                             \u001b[0mglobal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macc_value\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mglobal_acc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\python35\\lib\\site-packages\\tflearn\\helpers\\trainer.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, training_step, snapshot_epoch, snapshot_step, show_metric)\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0mtflearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         _, train_summ_str = self.session.run([self.train, self.summ_op],\n\u001b[0;32m--> 684\u001b[0;31m                                              feed_batch)\n\u001b[0m\u001b[1;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[1;31m# Retrieve loss value from summary string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    941\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                 \u001b[1;34m'which has shape %r'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m    944\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (32, 50) for Tensor 'InputData/X:0', which has shape '(?, 51)'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple example using LSTM recurrent neural network to classify IMDB\n",
    "sentiment dataset.\n",
    "References:\n",
    "    - Long Short Term Memory, Sepp Hochreiter & Jurgen Schmidhuber, Neural\n",
    "    Computation 9(8): 1735-1780, 1997.\n",
    "    - Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng,\n",
    "    and Christopher Potts. (2011). Learning Word Vectors for Sentiment\n",
    "    Analysis. The 49th Annual Meeting of the Association for Computational\n",
    "    Linguistics (ACL 2011).\n",
    "Links:\n",
    "    - http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n",
    "    - http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\"\"\"\n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "\n",
    "trainX = train['sub_seqs'].values\n",
    "trainY = train['sub_label'].values\n",
    "testX = test['sub_seqs'].values\n",
    "testY = test['sub_label'].values\n",
    "\n",
    "# Data preprocessing\n",
    "# Sequence padding\n",
    "trainX = pad_sequences(trainX, maxlen=max(train['seq_length'].values), value=0.)\n",
    "testX = pad_sequences(testX, maxlen=max(train['seq_length'].values), value=0.)\n",
    "# Converting labels to binary vectors\n",
    "trainY = to_categorical(trainY, nb_classes=len(tst.vocab))\n",
    "testY = to_categorical(testY, nb_classes=len(tst.vocab))\n",
    "\n",
    "# Network building\n",
    "net = tflearn.input_data([None, max(train['seq_length'].values)\n",
    "net = tflearn.embedding(net, input_dim=len(tst.vocab), output_dim=128)\n",
    "net = tflearn.lstm(net, 64, dropout=0.8)\n",
    "net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "net = tflearn.regression(net, optimizer='adam', learning_rate=0.001,\n",
    "                         loss='categorical_crossentropy')\n",
    "\n",
    "# Training\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(trainX, trainY, validation_set=(testX, testY), show_metric=True,\n",
    "          batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
