{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Scratch Pad</b>  \n",
    "User interaction data (ie # of comments, per subreddit etc)  - Can be used as word move weight/count  \n",
    "Architecture - WMD fed into net with features as top X closest users (ie user 1,8,56,123) which are ordered by their inherent embedding distances from a reference user (ie a default sub user), could also include furthest users, add NLP/content features for fine tuning suggestions from close users.  \n",
    "Need to deal with aging problem of recommender systems  \n",
    "Need to update network gefx file  \n",
    "identify users very far from you to introduce outside perspectives  \n",
    "Center of gravity of user comments over time of user account age  \n",
    "comment -> subreddit sequenced RNN, predict next subreddit to comment in?  \n",
    "Use subbeddit tags and related subreddits for network connections in embedding network  \n",
    "things learned - reading code from python packages  \n",
    "temporal EDA (distribution of post month/year)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ggplot import *\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from pyemd import emd\n",
    "import random\n",
    "from sklearn.metrics import euclidean_distances\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import SubRecommender\n",
    "from sklearn.preprocessing import normalize\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Introduction</h1>\n",
    "In this notebook, we explore a dataset compiled using Reddit's PRAW API in collecting historical user subbreddit comments. The goal of this analysis is to inform the development of a Recurrent Nueral Network model that can be used as a recommender system in recommending users new subreddits based on their historical subreddit commenting patterns.  \n",
    "\n",
    "<h2>Dataset</h2>\n",
    "\n",
    "The dataset was compiled using a python scrapper developed using Reddit's PRAW API. The raw data is a list of 3-tuples of [username,subreddit,utc timestamp]. Each row represents a single comments made by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/train_reddit_data.json','r') as data_file:    \n",
    "    reddit_data = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(reddit_data,columns=['user','subreddit','utc_stamp'])\n",
    "df['utc_stamp'] = pd.to_datetime(df['utc_stamp'],unit='s')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Unique Users = \" + str(len(df.groupby('user')['user'].nunique())))\n",
    "print(\"Unique Subreddits = \" + str(len(df.groupby('subreddit')['subreddit'].nunique())))\n",
    "print(\"Total User Comments = \" + str(df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Subreddit Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_subs = df.groupby(['user'])['subreddit'].nunique()\n",
    "plt.hist(user_subs.values, bins=100)\n",
    "plt.title(\"User vs Sub Counts Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_users = df.groupby(['subreddit'])['user'].nunique()\n",
    "data_tuple = pd.DataFrame([(sub,count) for sub,count in sub_users.items()],columns=[\"sub\",\"user_count\"])\n",
    "sorted_df = data_tuple.sort_values(by='user_count',ascending=False)\n",
    "sorted_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_summary = df.groupby(by=['user'])['user']\n",
    "plt.hist(user_summary.value_counts(), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(user_summary.value_counts(), bins=100)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users_vs_subs = []\n",
    "current_user = reddit_data[0][0]\n",
    "subs = []\n",
    "interaction_count = 0\n",
    "sub_discovery_time = []\n",
    "usr_sub_discovery_time = [0]\n",
    "user_subs_list = []\n",
    "for i,comment in enumerate(reddit_data):\n",
    "    if comment[0] != current_user:\n",
    "        user_subs_list = [comment[1]]\n",
    "        sub_discovery_time.append(usr_sub_discovery_time)\n",
    "        usr_sub_discovery_time = []\n",
    "        interaction_count = 0\n",
    "        users_vs_subs.append(len(subs))\n",
    "    elif comment[1] not in user_subs_list:\n",
    "        usr_sub_discovery_time.append(interaction_count)\n",
    "        interaction_count = 0\n",
    "        user_subs_list.append(comment[1])\n",
    "    if comment[1] not in subs:\n",
    "        subs.append(comment[1])\n",
    "    current_user = comment[0]\n",
    "    if comment[1] != reddit_data[i-1][1]:\n",
    "        interaction_count = interaction_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexes = np.arange(len(users_vs_subs))\n",
    "plt.plot(indexes,users_vs_subs)\n",
    "plt.title(\"Unique Subreddits vs User Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model Architecture</h1>\n",
    "\n",
    "The hypothesis of the recommender model is that, given an ordered sequence of user subreddit interactions, patterns will emerge that favour the discovery of paticular new subreddits given that historical user interaction sequence. The intuition is, that as users interact with the Reddit ecosystem, they discover new subreddits of interest, but these new discoveries are influenced by the communities they have previously been interacting with. We can then train a model to recognize these emergent subreddit discoveries based on users historical subreddit discovery patterns. When the model is presented with a new sequence of user interaction, it \"remembers\" other users that historically had similiar interaction habits and recommend their subreddits that the current user has yet to discover.  \n",
    "\n",
    "To build the training dataset, the subreddit interaction sequence for each user can be ordered and then split into chunks representing different periods of Reddit interaction and discovery. From each chunk, we can randomly remove a single subreddit from the interaction as the \"discovered\" subreddit and use it as our training label for the interaction sequences. This formulation brings with it a hyperparameter that will require tuning, namely the sequence size of each chunk of user interaction periods. There are also a couple design decisions needed that will create inherent assumptions in the model. This includes whether the labelled \"discovered\" subreddit should be randomly chosen from each interaction sequence, or should there be a more structured selection (ie. The latest subreddit to be interacted with in the sequence.) Also, should the sequences encode ever single comment interaction or only a subset, such as when the user switches between subreddits, as it may be possible to develop long sequences of the same subreddit repeatedly as the user engages in a conversation thread in a single subreddit over a short period. These design decisions will greatly influence the efficacy of the final model, and require exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disc_times = [usr_dts[-1] for usr_dts in sub_discovery_time if usr_dts and usr_dts[-1] > 1 and  usr_dts[-1] <100]\n",
    "plt.hist(disc_times , bins=90)\n",
    "plt.title(\"New Sub Discovery Time Steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.percentile(disc_times,95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "non_rep_interaction = [sum(usr_dts) for usr_dts in sub_discovery_time]\n",
    "plt.hist(non_rep_interaction , bins=90)\n",
    "plt.title(\"Total Non-Repeating User Interactions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(non_rep_interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flt_disc_times = [dt for usr_dts in sub_discovery_time for dt in usr_dts[50:] if dt < 50]\n",
    "plt.hist(flt_disc_times , bins=50)\n",
    "plt.title(\"Truncated New Sub Discovery Time Steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(flt_disc_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Network Embedding</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(open(\"data/embedding_network.html\"),\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_cords = {}\n",
    "nodes = soup.findAll('node')\n",
    "max_d =0\n",
    "\n",
    "for nd in nodes:\n",
    "    crds = nd.find('viz:position')\n",
    "    if float(crds['x']) > max_d:\n",
    "        max_d = float(crds['x'])\n",
    "    if float(crds['y']) > max_d:\n",
    "        max_d = float(crds['y'])\n",
    "\n",
    "for nd in nodes:\n",
    "    crds = nd.find('viz:position')\n",
    "    graph_cords[nd['label']] = [float(crds['x'])/max_d,float(crds['y'])/max_d]\n",
    "\n",
    "with open('data/embeddings.json','w') as data_file:    \n",
    "    json.dump(graph_cords,data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_cords['AdultToons']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model Testing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Training Data\n",
      "Building Vocab\n",
      "Building Training Sequences\n"
     ]
    }
   ],
   "source": [
    "import SubRecommender\n",
    "import json\n",
    "from collections import Counter\n",
    "tst = SubRecommender.SubRecommender('data/test_reddit_data.json',\n",
    "                                    sequence_chunk_size=15,min_seq_length=5,batch_size=256,min_count_thresh=10)\n",
    "train_df = tst.load_train_df()\n",
    "#tst.train(num_epochs=10,npartitions=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/user_comment_sequence_cache.json','r') as data_file:    \n",
    "    user_seqs = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tst_users = np.random.choice(list(user_seqs.keys()),2)\n",
    "seqs = [user_seqs[usr] for usr in tst_users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(tst.training_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tst.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tst.recommend_subs(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(181, 2609),\n",
       " (196, 2472),\n",
       " (122, 2350),\n",
       " (73, 2052),\n",
       " (293, 1617),\n",
       " (292, 1510),\n",
       " (208, 1430),\n",
       " (117, 1397),\n",
       " (218, 1281),\n",
       " (186, 1199)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = Counter(tst.training_labels)\n",
    "counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = counter.values()\n",
    "x = range(len(y))\n",
    "plt.bar(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.028071270254567366"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(1)[0][1]/len(tst.training_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(counter.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Baseline KNN Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017331022530329289"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "train,test = tst.split_train_test(train_df,0.8)\n",
    "X_train = np.array(train['sub_seqs'])\n",
    "y_train = np.array(train['sub_label']).astype(np.uint16)\n",
    "X_test = np.array(test['sub_seqs'])\n",
    "y_test =np.array(test['sub_label']).astype(np.uint16)\n",
    "\n",
    "enc = OneHotEncoder(n_values=tst.vocab_size)\n",
    "neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=tst.sequence_chunk_size, value=0.,padding='post')\n",
    "X_test = pad_sequences(X_test, maxlen=tst.sequence_chunk_size, value=0.,padding='post')\n",
    "\n",
    "enc.fit(X_train)\n",
    "\n",
    "X_train = csr_matrix(enc.transform(X_train).toarray())\n",
    "X_test = csr_matrix(enc.transform(X_test).toarray())\n",
    "\n",
    "clf = neigh.fit(X_train, y_train)\n",
    "\n",
    "accuracy_score(clf.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/user_comment_sequence_cache.json','r') as data_file:    \n",
    "    user_seqs = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "tst_users = np.random.choice(list(user_seqs.keys()),1)\n",
    "tst_data = {usr:user_seqs[usr] for usr in tst_users}\n",
    "recommendation_avg_signal_to_noise(tst_data,'knn',clf,enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recommendation_avg_signal_to_noise(user_data,model,clf,enc):\n",
    "    signal = 0\n",
    "    noise = 0\n",
    "    for usr,user_subs in user_data.items():\n",
    "        user_seqs = tst.build_training_sequences(user_subs)\n",
    "        print(user_seqs)\n",
    "        training_sequences = [data[0] for data in user_seqs]\n",
    "        training_labels = [data[1] for data in user_seqs]\n",
    "        print(str(len(training_sequences)) + \" Training Sequences\")\n",
    "        if training_sequences:\n",
    "            if model == 'knn':\n",
    "                X_test = pad_sequences(training_sequences, maxlen=tst.sequence_chunk_size, value=0.,padding='post')\n",
    "                X_test = csr_matrix(enc.transform(X_test).toarray())\n",
    "                preds = clf.predict(X_test)\n",
    "                for i,pred in enumerate(preds):\n",
    "                    print(\"Prediction = \" + str(pred))\n",
    "                    print(\"Actual Label = \" + str(training_labels[i]))\n",
    "                    if pred == training_labels[i]:\n",
    "                        signal = signal + 1\n",
    "                    else:\n",
    "                        noise = noise + 1\n",
    "    if noise == 0:\n",
    "        return 100\n",
    "    else:\n",
    "        return signal/noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Word Movers Distance</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pairwise_emd(user_A,user_B,graph_cords):\n",
    "    set_subs = [sub for sub in set(list(user_A.keys())+list(user_A.keys())) if sub in graph_cords.keys()]\n",
    "    sub_cords = np.array([graph_cords[sub] for sub in set_subs]) \n",
    "    A_interacts = np.array([user_A[sub] if sub in list(user_A.keys()) else 0 for sub in set_subs])\n",
    "    B_interacts = np.array([user_B[sub] if sub in list(user_B.keys()) else 0 for sub in set_subs])\n",
    "    euc_dists = euclidean_distances(sub_cords,sub_cords)\n",
    "    emd_dist = emd(A_interacts.astype(np.double), B_interacts.astype(np.double), euc_dists.astype(np.double))\n",
    "    return emd_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pairwise_emd(grouped['subreddit']['count']['-DEAD-'].sort_values(ascending=False),\n",
    "             grouped['subreddit']['count']['-Doomcrow-'].sort_values(ascending=False),graph_cords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
