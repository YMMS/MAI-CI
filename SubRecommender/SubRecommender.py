import rnn
import json
import pandas as pd
import numpy as np
from boltons.setutils import IndexedSet
from tflearn.data_utils import pad_sequences
from joblib import Parallel, delayed
from operator import itemgetter

np.random.seed(seed=42)

#List of top represented subreddits to filter out as training labels to avoid overbalancing training set in always recommended one of these ubiquitous subreddits
filter_list = ['AskReddit',
 'pics',
 'funny',
 'todayilearned',
 'worldnews',
 'videos',
 'gaming',
 'news',
 'gifs',
 'IAmA',
 'movies',
 'Showerthoughts',
 'politics',
 'aww',
 'WTF',
 'mildlyinteresting',
 'AdviceAnimals',
 'explainlikeimfive',
 'science',
 'Music',
 'technology',
 'television',
 'nottheonion',
 'LifeProTips',
 'OldSchoolCool',
 'tifu',
 'Jokes',
 'Futurology',
 'pcmasterrace',
 'The_Donald',
 'space',
 'dataisbeautiful',
 'sports',
 'books',
 'interestingasfuck',
 'food',
 'creepy',
 'pokemongo',
 'TwoXChromosomes',
 'BlackPeopleTwitter',
 'personalfinance',
 'atheism',
 'UpliftingNews',
 'bestof',
 'Documentaries',
 'Overwatch',
 'DIY',
 'askscience',
 'woahdude',
 'TumblrInAction']

def chunks(l, n):
    n = max(1, n)
    return (l[i:i+n] for i in range(0, len(l), n))

def normalize(lst):
    s = sum(lst)
    normed = [itm/s for itm in lst]
    normed[-1] = (normed[-1] + (1-sum(normed)))#pad last value with what ever difference neeeded to make sum to exactly 1
    return normed

class SubRecommender():

    """This is the main class for the Recommender System, which takes as input the training data file and optional data munging
     and modelling parameters to build the RNN model using the model structure developed in rnn.py.
     This class takes care of the data structuring from the reddit_scrapper.py raw data of [user,subbreddit,timestamp]
     tuples and tranforms them into training sequence/subreddit label data for the RNN. The class also provides the ability to load_file
     cached training data previously munged and saved to a json file that can be passed into the load_train_df method of the class.

    parameters:

    train_data_file - the file path to the json file containing the raw reddit data scrapped using reddit_scrapper.py

    sequence_chunk_size - the chunk size of the user subbreddit sequence data to split users reddit interactions into smaller sequences

    min_seq_length - the minimum user subreddit sequence length to be included as a training sequence

    min_count_thresh - used for developing the vocabulary, this represents the minimum count of user interactions that need to exist
    for a subreddit to be included in the vocabulary. If a sub has 50 example user interactions in the supplied dataset, but min_count_thresh 
    is 100, the subreddit is not included in the vocab and filtered out of the training data.

    """

    def __init__(self, train_data_file='',sequence_chunk_size = 50,min_seq_length=5,min_count_thresh=10):
        self.sequence_chunk_size = sequence_chunk_size
        self.train_data_file = train_data_file
        self.min_seq_length = min_seq_length
        self.training_sequences = []
        self.training_labels = []
        self.training_seq_lengths = []
        self.vocab = []
        self.min_count_thresh = min_count_thresh

    def load_train_df(self,load_file = ''):
        """This routine builds or loads a pandas dataframe of columns:
        sub_seq - the chunked sequences of users subreddit interactions 
        sub_label - the extracted subreddit label for each sequence of user subreddit sequences sub_seq
        seq_length - the length of the sequence

        If load_file is supplied, the dataframe is built directly from the load_files data, otherwise the supplied
        train_data_file data is used to build the training sequences and labels. Labels are generated by extracting a
        subreddit from a users chunked sequence that has not already been used as a label for this user and is not in the
        filter_list. A label is then selected from the list of potential labels by utilizing the vocab_probs as the weighted 
        random distribution to chose from, which gives a higher probability for rarer subreddits. This ensures a more even
        distribution of training labels for all subreddits in the vocabulary. The training sequences are then is
        the remaining subreddit interactions with the selected label removed.
        """
         
        print("Loading Training Data")
        if self.vocab == []:
            self.create_vocab()
        if load_file: #If load file is provided, load the pre-munged dataset into training dataframe
            print ("Data loaded from disk")
            cache_df = pd.read_json(load_file)
            #iterate of the provided pre-munged dataset to update the subreddit indexes to match the current models vocabulary
            for index, rw in cache_df.iterrows():
                if rw['sub_label'] in self.vocab:
                    self.training_sequences.append([self.vocab.index(sub) for sub in rw['sub_seqs'] if sub in self.vocab])
                    self.training_labels.append(self.vocab.index(rw['sub_label']))
                    self.training_seq_lengths.append(rw['seq_length'])
            return pd.DataFrame({'sub_seqs':self.training_sequences,'sub_label':self.training_labels,'seq_length':self.training_seq_lengths})
        #open the provided data file and user sequence cache
        with open(self.train_data_file,'r') as data_file:
            train_data = sorted(json.load(data_file),key=lambda x: (x[0], x[2])) #Sort by user then by utc time stamp
        with open("data/test_user_comment_sequence_cache.json",'r') as cache_file:
            cache_data = json.load(cache_file)

        self.training_sequences = []
        self.training_labels = []
        self.training_seq_lengths = []
        print("Building Training Sequences")
        #This loop iterates over all of the users in the dataset to build their unique subreddit sequences, which are loaded from the cache if user exists
        prev_usr = None
        for comment_data in train_data:
            current_usr = comment_data[0]
            if current_usr != prev_usr:#New user found in sorted comment data, begin sequence extraction for new user
                if prev_usr != None and prev_usr not in cache_data.keys():#dump sequences to cache for previous user if not in cache
                    cache_data[prev_usr] = usr_sub_seq
                if current_usr in cache_data.keys():
                    usr_sub_seq = cache_data[current_usr]
                else:
                    usr_sub_seq = [comment_data[1]] #initialize user sub sequence list with first sub for current user
                    past_sub = comment_data[1]
            else:#if still iterating through the same user, add new sub to sequence if not a repeat
                if comment_data[1] != past_sub:#Check that next sub comment is not a repeat of the last interacted with sub, filtering out repeated interactions
                    usr_sub_seq.append(comment_data[1])
                    past_sub = comment_data[1]
            prev_usr = current_usr #update previous user to being the current one before looping to next comment
        #dump the cache for newly sequenced users
        with open("data/test_user_comment_sequence_cache.json",'w') as cache_file:
            json.dump(cache_data,cache_file)
        #parallelized routine for building user sequence training data based on classes data munging parameters
        rslts = Parallel(n_jobs=6)(delayed(self.build_training_sequences)(usr) for usr in cache_data.values())
        #parse parallelized results
        self.training_sequences = [data[0] for seq_chunks in rslts for data in seq_chunks]
        self.training_labels = [data[1] for seq_chunks in rslts for data in seq_chunks]
        self.training_seq_lengths = [data[2] for seq_chunks in rslts for data in seq_chunks]
        train_df = pd.DataFrame({'sub_seqs':self.training_sequences,'sub_label':self.training_labels,'seq_length':self.training_seq_lengths})
        #store training df by converting subreddit indexes to real name for future loading
        cache_df = pd.DataFrame({'sub_seqs':[[self.vocab[vc_indx] for vc_indx in seq] for seq in self.training_sequences],
                                 'sub_label':[self.vocab[vc_indx] for vc_indx in self.training_labels],'seq_length':self.training_seq_lengths})
        cache_df.to_json("data/training_sequences/" + str(self.vocab_size) + "_" + str(self.sequence_chunk_size) + "_" + str(self.min_seq_length) + "_sequence_data.json")
        return train_df

    def build_training_sequences(self,usr_sub_seq):
        user_labels = []
        train_seqs = []
        #split user sub sequences into provided chunks of size sequence_chunk_size
        comment_chunks = chunks(usr_sub_seq,self.sequence_chunk_size)
        for chnk in comment_chunks:
            #for each chunk, filter out potential labels to select as training label, filter by the top subs filter list and any already utilized labels for this user
            filtered_subs = [self.vocab.index(sub) for sub in chnk if sub in self.filtered_vocab and self.vocab.index(sub) not in user_labels]
            if filtered_subs:
                #randomly select the label from filtered subs, using the vocab probability distribution to smooth out representation of subreddit labels
                filter_probs = normalize([self.vocab_probs[sub_indx] for sub_indx in filtered_subs])
                label = np.random.choice(filtered_subs,1,p=filter_probs)[0]
                user_labels.append(label)
                #build sequence by ensuring users sub exists in models vocabulary and filtering out the selected label for this subreddit sequence
                chnk_seq = [self.vocab.index(sub) for sub in chnk if sub in self.vocab and self.vocab.index(sub) != label] 
                if len(chnk_seq) > self.min_seq_length:#ensure resulting sequence length exeeds min_count_thresh
                    train_seqs.append([chnk_seq,label,len(chnk_seq)]) 
        return train_seqs

    def create_vocab(self):
        """This routine develops the models vocabulary using the supplied training data file and the filter list and min_count_thresh 
        parameters to filter out subs not meeting these requirements. The vocab_probs is also built, representing the inverse probability 
        of encounter a paticular subreddit in the given dataset, which is the used to bias the selection of rarer subreddits as labels to 
        smooth the distribution of training labels across all subreddits in the vocabulary"""
        print("Building Vocab")
        with open(self.train_data_file,'r') as data_file:
            train_data = json.load(data_file)
        df = pd.DataFrame(train_data,columns=['user','subreddit','utc_stamp'])
        self.vocab_counts = df["subreddit"].value_counts()
        tmp_vocab = [sub for sub,cnt in self.vocab_counts.items() if cnt >= self.min_count_thresh]
        total_counts = sum([self.vocab_counts[sub] for sub in tmp_vocab])
        inv_prob = [total_counts/self.vocab_counts[sub] for sub in tmp_vocab]
        self.vocab = ["Unseen-Sub"] + tmp_vocab
        self.filtered_vocab = [sub for sub in self.vocab if sub not in filter_list]
        tmp_vocab_probs = normalize(inv_prob)
        self.vocab_probs = [1-sum(tmp_vocab_probs)] + tmp_vocab_probs #force probs sum to 1 by adding differenc to "Unseen-sub" probability
        self.vocab_size = len(self.vocab)
        self.idx_to_vocab = dict(enumerate(self.vocab))
        self.vocab_to_idx = dict(zip(self.idx_to_vocab.values(), self.idx_to_vocab.keys()))
        return self.vocab

    def split_train_test(self,train_df,split_perc):
        train_len, test_len = np.floor(len(train_df)*split_perc), np.floor(len(train_df)*(1-split_perc))
        train, test = train_df.ix[:train_len-1], train_df.ix[train_len:train_len + test_len]
        return train, test 

    def train(self,load_file='',num_epochs=10,npartitions=6):
        if self.vocab == []:
            self.create_vocab()
        if self.training_labels == []:
            train_df = self.load_train_df(load_file)
        else:
            train_df = pd.DataFrame({'sub_seqs':self.training_sequences,'sub_label':self.training_labels,'seq_length':self.training_seq_lengths})
        train,test = self.split_train_test(train_df,0.8)
        print("Training Model")
        self.model = rnn.train_model(train,test,self.vocab_size,self.sequence_chunk_size,num_epochs=num_epochs,npartitions=npartitions)
        return self.model