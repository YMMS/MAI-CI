{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Scratch Pad</b>  \n",
    "User interaction data (ie # of comments, per subreddit etc)  - Can be used as word move weight/count  \n",
    "Architecture - WMD fed into net with features as top X closest users (ie user 1,8,56,123) which are ordered by their inherent embedding distances from a reference user (ie a default sub user), could also include furthest users, add NLP/content features for fine tuning suggestions from close users.  \n",
    "Need to deal with aging problem of recommender systems  \n",
    "Need to update network gefx file  \n",
    "identify users very far from you to introduce outside perspectives  \n",
    "Center of gravity of user comments over time of user account age  \n",
    "comment -> subreddit sequenced RNN, predict next subreddit to comment in?  \n",
    "Use subbeddit tags and related subreddits for network connections in embedding network  \n",
    "things learned - reading code from python packages  \n",
    "temporal EDA (distribution of post month/year)  \n",
    "Chrome extension to show visualization of selected users network compared to yours  \n",
    "explore dataset size over accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'rnn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0eb24d5bf4f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mSubRecommender\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\macle\\Desktop\\UPC Masters\\Semester 2\\CI\\SubRecommender\\SubRecommender.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mboltons\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mIndexedSet\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'rnn'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ggplot import *\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from pyemd import emd\n",
    "import random\n",
    "from sklearn.metrics import euclidean_distances\n",
    "import tensorflow as tf\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "import tflearn\n",
    "import SubRecommender\n",
    "from sklearn.preprocessing import normalize\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Introduction</h1>\n",
    "In this notebook, we explore a dataset compiled using Reddit's PRAW API in collecting historical user subbreddit comments. The goal of this analysis is to inform the development of a Recurrent Nueral Network model that can be used as a recommender system in recommending users new subreddits based on their historical subreddit commenting patterns.  \n",
    "\n",
    "<h2>Dataset</h2>\n",
    "\n",
    "The dataset was compiled using a python scrapper developed using Reddit's PRAW API. The raw data is a list of 3-tuples of [username,subreddit,utc timestamp]. Each row represents a single comment made by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/train_reddit_data.json','r') as data_file:    \n",
    "    reddit_data = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(reddit_data,columns=['user','subreddit','utc_stamp'])\n",
    "df['utc_stamp'] = pd.to_datetime(df['utc_stamp'],unit='s')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Unique Users = \" + str(len(df.groupby('user')['user'].nunique())))\n",
    "print(\"Unique Subreddits = \" + str(len(df.groupby('subreddit')['subreddit'].nunique())))\n",
    "print(\"Total User Comments = \" + str(df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Subreddit Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_subs = df.groupby(['user'])['subreddit'].nunique()\n",
    "plt.hist(user_subs.values, bins=100)\n",
    "plt.title(\"User vs Sub Counts Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_users = df.groupby(['subreddit'])['user'].nunique()\n",
    "data_tuple = pd.DataFrame([(sub,count) for sub,count in sub_users.items()],columns=[\"sub\",\"user_count\"])\n",
    "sorted_df = data_tuple.sort_values(by='user_count',ascending=False)\n",
    "sorted_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_summary = df.groupby(by=['user'])['user']\n",
    "plt.hist(user_summary.value_counts(), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(user_summary.value_counts(), bins=100)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users_vs_subs = []\n",
    "current_user = reddit_data[0][0]\n",
    "subs = []\n",
    "interaction_count = 0\n",
    "sub_discovery_time = []\n",
    "usr_sub_discovery_time = [0]\n",
    "user_subs_list = []\n",
    "for i,comment in enumerate(reddit_data):\n",
    "    if comment[0] != current_user:\n",
    "        user_subs_list = [comment[1]]\n",
    "        sub_discovery_time.append(usr_sub_discovery_time)\n",
    "        usr_sub_discovery_time = []\n",
    "        interaction_count = 0\n",
    "        users_vs_subs.append(len(subs))\n",
    "    elif comment[1] not in user_subs_list:\n",
    "        usr_sub_discovery_time.append(interaction_count)\n",
    "        interaction_count = 0\n",
    "        user_subs_list.append(comment[1])\n",
    "    if comment[1] not in subs:\n",
    "        subs.append(comment[1])\n",
    "    current_user = comment[0]\n",
    "    if comment[1] != reddit_data[i-1][1]:\n",
    "        interaction_count = interaction_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexes = np.arange(len(users_vs_subs))\n",
    "plt.plot(indexes,users_vs_subs)\n",
    "plt.title(\"Unique Subreddits vs User Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model Architecture</h1>\n",
    "\n",
    "The hypothesis of the recommender model is that, given an ordered sequence of user subreddit interactions, patterns will emerge that favour the discovery of paticular new subreddits given that historical user interaction sequence. The intuition is, that as users interact with the Reddit ecosystem, they discover new subreddits of interest, but these new discoveries are influenced by the communities they have previously been interacting with. We can then train a model to recognize these emergent subreddit discoveries based on users historical subreddit discovery patterns. When the model is presented with a new sequence of user interaction, it \"remembers\" other users that historically had similiar interaction habits and recommend their subreddits that the current user has yet to discover.  \n",
    "\n",
    "To build the training dataset, the subreddit interaction sequence for each user can be ordered and then split into chunks representing different periods of Reddit interaction and discovery. From each chunk, we can randomly remove a single subreddit from the interaction as the \"discovered\" subreddit and use it as our training label for the interaction sequences. This formulation brings with it a hyperparameter that will require tuning, namely the sequence size of each chunk of user interaction periods. \n",
    "\n",
    "There are also a couple of design decisions needed that will create inherent assumptions in the model. This includes whether the labelled \"discovered\" subreddit should be randomly chosen from each interaction sequence, or should there be a more structured selection. The proposed model utilizes the distribution of subreddits existing in the dataset to weight the random selection of a subreddit as the sequence label, which gives a higher probability of selection to rarer subreddits. This will smoothen the distribution of training labels across the models vocabulary of subreddits in the dataset. Also, each users interaction sequence has been compressed to only represent the sequence of non-repeating subreddits, to eliminate the repeatative structure of users constantly commenting in a single subreddit, while providing information of the users habits in the reddit ecosystem more generally, allowing the model to distinguish broader patterns from the compressed sequences.\n",
    "\n",
    "These subreddit sequence/subreddit label pairs are then passed to various RNN architectures (shallow and deep LSTM/GRU networks) with an exploration of hyperparamter optimization to select the optimal model for recommending new subreddits of interest to reddit users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disc_times = [usr_dts[-1] for usr_dts in sub_discovery_time if usr_dts and usr_dts[-1] > 1 and  usr_dts[-1] <100]\n",
    "plt.hist(disc_times , bins=90)\n",
    "plt.title(\"New Sub Discovery Time Steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.percentile(disc_times,95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "non_rep_interaction = [sum(usr_dts) for usr_dts in sub_discovery_time]\n",
    "plt.hist(non_rep_interaction , bins=90)\n",
    "plt.title(\"Total Non-Repeating User Interactions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(non_rep_interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flt_disc_times = [dt for usr_dts in sub_discovery_time for dt in usr_dts[50:] if dt < 50]\n",
    "plt.hist(flt_disc_times , bins=50)\n",
    "plt.title(\"Truncated New Sub Discovery Time Steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(flt_disc_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model Testing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import SubRecommender\n",
    "import json\n",
    "from collections import Counter\n",
    "tst = SubRecommender.SubRecommender('data/train_reddit_data.json',\n",
    "                                    sequence_chunk_size=15,min_seq_length=5,min_count_thresh=100)\n",
    "train_df = tst.load_train_df(load_file=\"data/training_sequences/329_15_5_sequence_data.json\")\n",
    "tst.train(num_epochs=10,npartitions=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_counter = Counter(tst.training_labels)\n",
    "label_counter.most_common(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(label_counter.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Baseline KNN Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "train,test = tst.split_train_test(train_df,0.8)\n",
    "X_train = np.array(train['sub_seqs'])\n",
    "y_train = np.array(train['sub_label']).astype(np.uint16)\n",
    "X_test = np.array(test['sub_seqs'])\n",
    "y_test =np.array(test['sub_label']).astype(np.uint16)\n",
    "\n",
    "enc = OneHotEncoder(n_values=tst.vocab_size)\n",
    "neigh = KNeighborsClassifier(n_neighbors=1,n_jobs=6)\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=tst.sequence_chunk_size, value=0.,padding='post')\n",
    "X_test = pad_sequences(X_test, maxlen=tst.sequence_chunk_size, value=0.,padding='post')\n",
    "\n",
    "enc.fit(X_train)\n",
    "\n",
    "X_train = csr_matrix(enc.transform(X_train).toarray())\n",
    "X_test = csr_matrix(enc.transform(X_test).toarray())\n",
    "\n",
    "clf = neigh.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_score(clf.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def recommendation_accuracy(user_data,model,clf,enc=None):\n",
    "    accuracies = []\n",
    "    for usr,user_subs in user_data.items():\n",
    "        user_seqs = tst.build_training_sequences(user_subs)\n",
    "        training_sequences = [data[0] for data in user_seqs]\n",
    "        training_labels = [data[1] for data in user_seqs]\n",
    "        X_test = pad_sequences(training_sequences, maxlen=tst.sequence_chunk_size, value=0.,padding='post')\n",
    "        if training_sequences:\n",
    "            if model == 'knn':\n",
    "                X_test = csr_matrix(enc.transform(X_test).toarray())\n",
    "                recs = set(clf.predict(X_test))\n",
    "            elif model == 'rnn':\n",
    "                sub_probs = clf.predict(X_test)\n",
    "                recs = set([probs.index(max(probs)) for probs in sub_probs])\n",
    "            elif model == 'most_common':\n",
    "                recs = [clf]\n",
    "            accuracy = sum([1 if rec in training_labels else 0 for rec in recs])/len(recs)\n",
    "            accuracies.append(accuracy)        \n",
    "    return np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/test_user_comment_sequence_cache.json','r') as data_file:    \n",
    "    user_seqs = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recommendation_accuracy(user_seqs,'rnn',tst.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recommendation_accuracy(user_seqs,'most_common',4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Word Movers Distance</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pairwise_emd(user_A,user_B,graph_cords):\n",
    "    set_subs = [sub for sub in set(list(user_A.keys())+list(user_A.keys())) if sub in graph_cords.keys()]\n",
    "    sub_cords = np.array([graph_cords[sub] for sub in set_subs]) \n",
    "    A_interacts = np.array([user_A[sub] if sub in list(user_A.keys()) else 0 for sub in set_subs])\n",
    "    B_interacts = np.array([user_B[sub] if sub in list(user_B.keys()) else 0 for sub in set_subs])\n",
    "    euc_dists = euclidean_distances(sub_cords,sub_cords)\n",
    "    emd_dist = emd(A_interacts.astype(np.double), B_interacts.astype(np.double), euc_dists.astype(np.double))\n",
    "    return emd_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pairwise_emd(grouped['subreddit']['count']['-DEAD-'].sort_values(ascending=False),\n",
    "             grouped['subreddit']['count']['-Doomcrow-'].sort_values(ascending=False),graph_cords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
